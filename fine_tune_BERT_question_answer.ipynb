{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tune_BERT_question_answer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangguanheng66/tutorials/blob/bert_question_answer/fine_tune_BERT_question_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVptaygFi5dk"
      },
      "source": [
        "%%shell\n",
        "\n",
        "rm -r /usr/local/lib/python3.6/dist-packages/torch*\n",
        "pip install numpy\n",
        "pip install --pre torch torchtext -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxB-0w5FFXn-",
        "outputId": "a091ab1b-a2bc-4c05-9313-a69bf6d3d469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "print(torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AZuMstAGT1m"
      },
      "source": [
        "# Fine-tuning BERT model for Question-Answer with Torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6SWfPD4Gbpr"
      },
      "source": [
        "Recently, we release a new torchtext library to address a few issues from OSS community ([torchtext 0.7.0 release note](https://github.com/pytorch/text/releases/tag/v0.7.0-rc3) and 0.8.0 release note). To accelerate research, the new torchtext library will provide reusable, orthogonal, correct, and performant building blocks  (Vectors, Vocab, Tokenizer) for cutting-edge experimentation based on deep knowledge of the NLP research and communities. In addition, we deeply integrate torchtext with broad range of PyTorch capabilities, such as Just-in-Time (JIT), quantization, distributed, and mobile, to enable seamless research-to-production for core end-to-end applications.\n",
        "\n",
        "In this tutorial, we are going to fine-tune a pretrained BERT model for question-answer task with the new library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DylRHo85Gw0F"
      },
      "source": [
        "## Step 1: Prepare datasets\n",
        "\n",
        "We have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer backed by regular expression, and sentencepiece. Those are the basic data processing building blocks for raw text string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "splH7rFPG-En"
      },
      "source": [
        "### Tokenizer-vocabulary data processing pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4vHJzqIDDQ6"
      },
      "source": [
        "Download a vocab text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQfnnCWkC6GZ",
        "outputId": "f406b33f-6199-4103-986e-acb24b9cc26f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "%%shell\n",
        "rm bert_vocab.txt\n",
        "wget https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/bert_vocab.txt "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'bert_vocab.txt': No such file or directory\n",
            "--2020-10-01 16:30:13--  https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/bert_vocab.txt\n",
            "Resolving pytorch.s3.amazonaws.com (pytorch.s3.amazonaws.com)... 52.216.102.11\n",
            "Connecting to pytorch.s3.amazonaws.com (pytorch.s3.amazonaws.com)|52.216.102.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 923169 (902K) [text/plain]\n",
            "Saving to: ‘bert_vocab.txt’\n",
            "\n",
            "bert_vocab.txt      100%[===================>] 901.53K  2.30MB/s    in 0.4s    \n",
            "\n",
            "2020-10-01 16:30:14 (2.30 MB/s) - ‘bert_vocab.txt’ saved [923169/923169]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea0U_5KVDWj_"
      },
      "source": [
        "Prepare data pipeline for the question-answer dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu5FmCrLDc6I"
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    basic_english_normalize,\n",
        "    TextSequentialTransforms,\n",
        ")\n",
        "from torchtext.experimental.vocab import vocab_from_file\n",
        "with open('bert_vocab.txt', 'r') as f:\n",
        "  vocab = vocab_from_file(f)\n",
        "text_pipeline = TextSequentialTransforms(basic_english_normalize(), vocab)\n",
        "pos_pipeline = lambda x: torch.tensor(x, dtype=torch.long)\n",
        "qa_data_pipeline= {'context': text_pipeline, 'question': text_pipeline,\n",
        "                   'answers': text_pipeline, 'ans_pos': pos_pipeline}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2CeoYGIDd3C"
      },
      "source": [
        "The dataset in `torchtext.experimental.datasets.raw` returns iterators which yield the raw data. In this way, users can definte the custom data processing pipelines and work on the raw data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgxs-nrIDfsc"
      },
      "source": [
        "from torchtext.experimental.datasets.raw import SQuAD1\n",
        "train, dev = SQuAD1()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4PzOfq_Dusk"
      },
      "source": [
        "Materialize the raw SQuAD data iterators. Pass the data and data processing pipelines (a.k.a. transforms) to the question answer dataset abstraction. `QuestionAnswerDataset` is an abstraction ([link](https://github.com/pytorch/text/blob/467ee98faba8e00b0e6acbf3132a723e08f36859/torchtext/experimental/datasets/question_answer.py#L12)) that applies the user-defined transform pipelines to the raw question-answer data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjLe1KKbreLz"
      },
      "source": [
        "from torchtext.experimental.datasets.question_answer import QuestionAnswerDataset\n",
        "train_data = QuestionAnswerDataset(list(train), vocab, qa_data_pipeline)\n",
        "dev_data = QuestionAnswerDataset(list(dev), vocab, qa_data_pipeline)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKXuIDHaHNss"
      },
      "source": [
        "### (Optional for tutorial) Word-vector embedding data processing pipeline\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. FastText and GloVe are well established baseline word vectors in the NLP community. In the new torchtext library, a Vector object supports the mapping between tokens and their corresponding vector representation (i.e. word embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSillz6wHTDY"
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    basic_english_normalize,\n",
        "    TextSequentialTransforms,\n",
        ")\n",
        "from torchtext.experimental.vectors import FastText\n",
        "vector = FastText()\n",
        "word_vector_pipeline = TextSequentialTransforms(basic_english_normalize(), vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITgbca8OHY-p"
      },
      "source": [
        "### (Optional for tutorial) SentencePiece data processing pipeline\n",
        "\n",
        "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. For sentencepiece transforms in torchtext, both subword units (e.g., byte-pair-encoding (BPE) ) and unigram language model are supported.\n",
        "\n",
        "Here is an example to apply SentencePiece transform to build a Language Modeling dataset. Although the pretrained BERT model was generated on a different vocabulary, the follow LM dataset with the SentencePiece transform can be used to train a masked language model task (described in the BERT paper) from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNHW2FxwHfeZ",
        "outputId": "e406a367-7bea-4286-e5cb-2caf7f1c437f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    PRETRAINED_SP_MODEL,\n",
        "    sentencepiece_processor,\n",
        ")\n",
        "from torchtext.utils import download_from_url\n",
        "spm_filepath = download_from_url(PRETRAINED_SP_MODEL['text_unigram_25000'])\n",
        "spm_transform = sentencepiece_processor(spm_filepath)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text_unigram_25000.model: 100%|██████████| 678k/678k [00:00<00:00, 2.15MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF1ptRF02CEb"
      },
      "source": [
        "Check out raw WikiText2 dataset as an iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5mlrLpi2A55",
        "outputId": "ea1bf093-ee6a-4328-bcaa-89ab803167cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torchtext.experimental.datasets.raw import WikiText2\n",
        "train_iter, test_iter, valid_iter = WikiText2()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 8.54MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYQoLhy22Vht"
      },
      "source": [
        "Add WikiText2 data and data processing pipeline (a.k.a. transform) to the language modeling dataset abstraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ak9JlRJ2WMD",
        "outputId": "fda8542b-2c30-4c63-f8d5-08c4165c966f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torchtext.experimental.datasets.language_modeling import LanguageModelingDataset\n",
        "wikitext2_train = LanguageModelingDataset(list(train_iter), None, spm_transform, False)\n",
        "wikitext2_test = LanguageModelingDataset(list(test_iter), None, spm_transform, False)\n",
        "wikitext2_valid = LanguageModelingDataset(list(valid_iter), None, spm_transform, False)\n",
        "len(wikitext2_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36718"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKyPu6nfHigd"
      },
      "source": [
        "### JIT support for the data processing pipeline\n",
        "\n",
        "The new building blocks in torchtext library is compatible with `torch.jit.script`. TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency. The data processing pipelines above can be converted and run on the JIT mode without Python dependency\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO6FWVGnHnYU"
      },
      "source": [
        "text_pipeline = text_pipeline.to_ivalue()\n",
        "jit_text_pipeline = torch.jit.script(text_pipeline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmqHKb28HlS3"
      },
      "source": [
        "## Step 2: Data Iterator\n",
        "\n",
        "The PyTorch data loading utility is the [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class. It works with a map-style dataset that implements the __getitem__() and __len__() protocols, and represents a map from indices/keys to data sampl*e*s. Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgcfY4fhH-Qb"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# [TODO] integrate with torchtext.experimental.transforms.PadTransform\n",
        "# Need to land https://github.com/pytorch/text/pull/952\n",
        "\n",
        "cls_id = vocab(['<cls>'])\n",
        "sep_id = vocab(['<sep>'])\n",
        "pad_id = vocab(['<pad>'])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    seq_list, ans_pos_list, tok_type = [], [], []\n",
        "    for (_context, _question, _answers, _ans_pos) in batch:\n",
        "        _context, _question = torch.tensor(_context), torch.tensor(_question)\n",
        "        qa_item = torch.cat((torch.tensor(cls_id), _question, torch.tensor(sep_id),\n",
        "                             _context, torch.tensor(sep_id)))\n",
        "        seq_list.append(qa_item)\n",
        "        pos_list = [pos + _question.size(0) + 2 for pos in _ans_pos]\n",
        "        ans_pos_list.append(pos_list)\n",
        "        tok_type.append(torch.cat((torch.zeros((_question.size(0) + 2)),\n",
        "                                   torch.ones((_context.size(0) + 1)))))\n",
        "    _ans_pos_list = [torch.stack(list(pos)) for pos in zip(*ans_pos_list)]\n",
        "    target_start_pos, target_end_pos = _ans_pos_list[0].split(1, dim=-1)\n",
        "    target_start_pos = target_start_pos.squeeze(-1)\n",
        "    target_end_pos = target_end_pos.squeeze(-1)\n",
        "    seq_list = torch.nn.utils.rnn.pad_sequence(seq_list, batch_first=True, padding_value=float(pad_id[0]))\n",
        "    seq_list = seq_list.long().t().contiguous()\n",
        "    tok_type = torch.nn.utils.rnn.pad_sequence(tok_type, batch_first=True, padding_value=1.0)\n",
        "    tok_type = tok_type.long().t().contiguous()\n",
        "    return seq_list.to(device), target_start_pos.to(device), target_end_pos.to(device), tok_type.to(device)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "dev_dataloader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH0aoiGxH0P4"
      },
      "source": [
        "## Step 3: Model for Question-Answer Task\n",
        "\n",
        "A BERT model was pretrained with the masked language modeling task and next-sentence task according to the paper - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).  torchtext.datasets.WikiText103 and BookCorpus were used to pre-trained the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-_8lstuIMci"
      },
      "source": [
        "Here is the layout of the model for the question-answer task. On the top of the BERT model, there is a linear layer to project the position of the answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVMQBuOFIO7R"
      },
      "source": [
        "class QuestionAnswerTask(torch.nn.Module):\n",
        "    \"\"\"Contain a pretrain BERT model and a linear layer.\"\"\"\n",
        "\n",
        "    def __init__(self, bert_model):\n",
        "        super(QuestionAnswerTask, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.activation = torch.nn.GELU()\n",
        "        self.qa_span = torch.nn.Linear(bert_model.ninp, 2)\n",
        "\n",
        "    def forward(self, src, token_type_input):\n",
        "        output = self.bert_model(src, token_type_input)\n",
        "        # transpose output (S, N, E) to (N, S, E)\n",
        "        output = output.transpose(0, 1)\n",
        "        output = self.activation(output)\n",
        "        pos_output = self.qa_span(output)\n",
        "        start_pos, end_pos = pos_output.split(1, dim=-1)\n",
        "        start_pos = start_pos.squeeze(-1)\n",
        "        end_pos = end_pos.squeeze(-1)\n",
        "        return start_pos, end_pos"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9wR51SxKRp_"
      },
      "source": [
        " The pretrained model is available here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F6QIf30Id8p"
      },
      "source": [
        "%%shell\n",
        "rm ns_bert.pt, model.py\n",
        "wget https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/ns_bert.pt\n",
        "md5sum -c <<<\"f14abe2424ea321e66a82407ddab2dd4 ns_bert.pt\"\n",
        "wget https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/model.py\n",
        "md5sum -c <<<\"cbf74f9e864a988f25d09b51f5080168 model.py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jMwRkP1InFU"
      },
      "source": [
        "and can be loaded to the question answer task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1vNeEkQIH1O"
      },
      "source": [
        "from model import BertModel\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size, emsize, nhead, nhid, nlayers, dropout = 99230, 768, 12, 3072, 12, 0.2\n",
        "pretrained_bert = BertModel(vocab_size, emsize, nhead, nhid, nlayers, dropout)\n",
        "pretrained_bert.load_state_dict(torch.load('ns_bert.pt', map_location=device))\n",
        "qa_model = QuestionAnswerTask(pretrained_bert).to(device)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV9ylnHCIRTh"
      },
      "source": [
        "## Step 4: Fine-tuning the Model\n",
        "\n",
        "Then, we fine-tune the BERT model with the question-answer task based on the SQuAD1 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMa0bgCUIWNe"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def fine_tune(model, dataloader, optimizer, criterion, batch_size, device):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    log_interval = 20\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (seq_input, target_start_pos, target_end_pos, tok_type) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        # print(seq_input.size(), tok_type.size())\n",
        "        start_pos, end_pos = model(seq_input, token_type_input=tok_type)\n",
        "        loss = (criterion(start_pos, target_start_pos) + criterion(end_pos, target_end_pos)) / 2\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | '\n",
        "                  'ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(epoch, idx,\n",
        "                                                      len(dataloader) // batch_size,\n",
        "                                                      scheduler.get_last_lr()[0],\n",
        "                                                      elapsed * 1000 / log_interval,\n",
        "                                                      cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model, dataloader, optimizer, criterion, batch_size, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ans_pred_tokens_samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (seq_input, target_start_pos, target_end_pos, tok_type) in enumerate(dataloader):\n",
        "            start_pos, end_pos = model(seq_input, token_type_input=tok_type)\n",
        "            loss = (criterion(start_pos, target_start_pos)\n",
        "                    + criterion(end_pos, target_end_pos)) / 2\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt8S9g2W2hus"
      },
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 0.5  # learning rate\n",
        "BATCH_SIZE = 72 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(qa_model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "eval_loss = None\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    fine_tune(qa_model, train_dataloader, optimizer, criterion, BATCH_SIZE, device)\n",
        "    _loss = evaluate(qa_model, dev_dataloader, optimizer, criterion, BATCH_SIZE, device)\n",
        "    if eval_loss is not None and _loss > eval_loss:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       eval_loss = _loss\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid loss {:5.2f} | '.format(epoch, (time.time() - epoch_start_time),\n",
        "                                         _loss))\n",
        "    print('-' * 89)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}