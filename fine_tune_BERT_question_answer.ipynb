{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tune_BERT_question_answer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangguanheng66/tutorials/blob/bert_question_answer/fine_tune_BERT_question_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVptaygFi5dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "rm -r /usr/local/lib/python3.6/dist-packages/torch*\n",
        "pip install numpy\n",
        "pip install --pre torch torchvision torchtext -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxB-0w5FFXn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AZuMstAGT1m",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning BERT model for Question-Answer with Torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6SWfPD4Gbpr",
        "colab_type": "text"
      },
      "source": [
        "Recently, we release a new torchtext library to address a few issues from OSS community ([torchtext 0.7.0 release note](https://github.com/pytorch/text/releases/tag/v0.7.0-rc3) and 0.8.0 release note). To accelerate research, the new torchtext library will provide reusable, orthogonal, correct, and performant building blocks  (Vectors, Vocab, Tokenizer) for cutting-edge experimentation based on deep knowledge of the NLP research and communities. In addition, we deeply integrate torchtext with broad range of PyTorch capabilities, such as Just-in-Time (JIT), quantization, distributed, and mobile, to enable seamless research-to-production for core end-to-end applications.\n",
        "\n",
        "In this tutorial, we are going to fune-tune a pretrained BERT model for question-answer task with the new library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DylRHo85Gw0F",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Prepare datasets\n",
        "\n",
        "We have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer backed by regular expression, and sentencepiece. Those are the basic data processing building blocks for raw text string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "splH7rFPG-En",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizer-vocabulary data processing pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4vHJzqIDDQ6",
        "colab_type": "text"
      },
      "source": [
        "Download a vocab text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQfnnCWkC6GZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "rm *.txt\n",
        "wget https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/bert_vocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea0U_5KVDWj_",
        "colab_type": "text"
      },
      "source": [
        "Prepare data pipeline for the question-answer dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu5FmCrLDc6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    basic_english_normalize,\n",
        "    TextSequentialTransforms,\n",
        ")\n",
        "from torchtext.experimental.vocab import vocab_from_file\n",
        "vocab = vocab_from_file(open('bert_vocab.txt', 'r'))\n",
        "text_pipeline = TextSequentialTransforms(basic_english_normalize(), vocab)\n",
        "pos_pipeline = lambda x: torch.tensor(x, dtype=torch.long)\n",
        "qa_data_pipeline= {'context': text_pipeline, 'question': text_pipeline,\n",
        "                   'answers': text_pipeline, 'ans_pos': pos_pipeline}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2CeoYGIDd3C",
        "colab_type": "text"
      },
      "source": [
        "Check out raw SQuAD dataset as an iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgxs-nrIDfsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.experimental.datasets.raw import SQuAD1\n",
        "train, dev = SQuAD1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4PzOfq_Dusk",
        "colab_type": "text"
      },
      "source": [
        "Add SQuAD data and data processing pipelines (a.k.a. transforms) to the question answer dataset abstraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjLe1KKbreLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.experimental.datasets.question_answer import QuestionAnswerDataset\n",
        "train_data = QuestionAnswerDataset([item for item in train], vocab, qa_data_pipeline)\n",
        "dev_data = QuestionAnswerDataset([item for item in dev], vocab, qa_data_pipeline)\n",
        "# train_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKXuIDHaHNss",
        "colab_type": "text"
      },
      "source": [
        "### (Optional for tutorial) Word-vector embedding data processing pipeline\n",
        "\n",
        "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. FastText and GloVe are well established baseline word vectors in the NLP community. In the new torchtext library, a Vector object supports the mapping between tokens and their corresponding vector representation (i.e. word embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSillz6wHTDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    basic_english_normalize,\n",
        "    TextSequentialTransforms,\n",
        ")\n",
        "from torchtext.experimental.vectors import FastText\n",
        "vector = FastText()\n",
        "word_vector_pipeline = TextSequentialTransforms(basic_english_normalize(), vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITgbca8OHY-p",
        "colab_type": "text"
      },
      "source": [
        "### (Optional for tutorial) SentencePiece data processing pipeline\n",
        "\n",
        "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. For sentencepiece transforms in torchtext, both subword units (e.g., byte-pair-encoding (BPE) ) and unigram language model are supported.\n",
        "\n",
        "Here is an example to apply SentencePiece transform to build a Language Modeling dataset. Although the pretrained BERT model was generated on a different vocabulary, the follow LM dataset with the SentencePiece transform can be used to train a masked language model task (described in the BERT paper) from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNHW2FxwHfeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need to land https://github.com/pytorch/text/pull/916\n",
        "from torchtext.experimental.transforms import (\n",
        "    load_pretrained_sp_model,\n",
        "    sentencepiece_processor,\n",
        ")\n",
        "spm_filepath = load_pretrained_sp_model()[1]\n",
        "spm_transform = sentencepiece_processor(spm_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF1ptRF02CEb",
        "colab_type": "text"
      },
      "source": [
        "Check out raw WikiText2 dataset as an iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5mlrLpi2A55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.experimental.datasets.raw import WikiText2\n",
        "train_iter, test_iter, valid_iter = WikiText2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYQoLhy22Vht",
        "colab_type": "text"
      },
      "source": [
        "Add WikiText2 data and data processing pipeline (a.k.a. transform) to the language modeling dataset abstraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ak9JlRJ2WMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.experimental.datasets.language_modeling import LanguageModelingDataset\n",
        "wikitext2_train = LanguageModelingDataset(list(train_iter), None, spm_transform, False)\n",
        "wikitext2_test = LanguageModelingDataset(list(test_iter), None, spm_transform, False)\n",
        "wikitext2_valid = LanguageModelingDataset(list(valid_iter), None, spm_transform, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKyPu6nfHigd",
        "colab_type": "text"
      },
      "source": [
        "### JIT support for the data processing pipeline\n",
        "\n",
        "The new building blocks in torchtext library is compatible with torch.jit.script. TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency. The data processing pipelines above can be converted and run on the JIT mode without Python dependency\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO6FWVGnHnYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jit_text_pipeline = torch.jit.script(text_pipeline.to_ivalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmqHKb28HlS3",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Data Iterator\n",
        "\n",
        "The PyTorch data loading utility is the [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class. It works with a map-style dataset that implements the __getitem__() and __len__() protocols, and represents a map from indices/keys to data sampl*e*s. Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgcfY4fhH-Qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# [TODO] integrate with torchtext.experimental.transforms.PadTransform\n",
        "# Need to land https://github.com/pytorch/text/pull/952\n",
        "\n",
        "cls_id = vocab(['<cls>'])\n",
        "sep_id = vocab(['<sep>'])\n",
        "pad_id = vocab(['<pad>'])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def collate_batch(batch):\n",
        "    seq_list, ans_pos_list, tok_type = [], [], []\n",
        "    for (_context, _question, _answers, _ans_pos) in batch:\n",
        "        _context, _question = torch.tensor(_context), torch.tensor(_question)\n",
        "        qa_item = torch.cat((torch.tensor(cls_id), _question, torch.tensor(sep_id),\n",
        "                             _context, torch.tensor(sep_id)))\n",
        "        seq_list.append(qa_item)\n",
        "        pos_list = [pos + _question.size(0) + 2 for pos in _ans_pos]\n",
        "        ans_pos_list.append(pos_list)\n",
        "        tok_type.append(torch.cat((torch.zeros((_question.size(0) + 2)),\n",
        "                                   torch.ones((_context.size(0))))))\n",
        "    _ans_pos_list = [torch.stack(list(pos)) for pos in zip(*ans_pos_list)]\n",
        "    seq_list = torch.nn.utils.rnn.pad_sequence(seq_list, batch_first=True, padding_value=float(pad_id[0]))\n",
        "    tok_type = torch.nn.utils.rnn.pad_sequence(tok_type, batch_first=True, padding_value=1.0)\n",
        "    return seq_list.long(), _ans_pos_list, tok_type.long()\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hONVWtp9eKah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for batch in dataloader:\n",
        "  print(batch)\n",
        "  break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH0aoiGxH0P4",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Model for Question-Answer Task\n",
        "\n",
        "A BERT model was pretrained with the masked language modeling task and next-sentence task according to the paper - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).  torchtext.datasets.WikiText103 and BookCorpus were used to pre-trained the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-_8lstuIMci",
        "colab_type": "text"
      },
      "source": [
        "Here is the layout of the model for the question-answer task. On the top of the BERT model, there is a linear layer to project the position of the answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVMQBuOFIO7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuestionAnswerTask(torch.nn.Module):\n",
        "    \"\"\"Contain a pretrain BERT model and a linear layer.\"\"\"\n",
        "\n",
        "    def __init__(self, bert_model):\n",
        "        super(QuestionAnswerTask, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.activation = nn.functional.gelu\n",
        "        self.qa_span = nn.Linear(bert_model.ninp, 2)\n",
        "\n",
        "    def forward(self, src, token_type_input):\n",
        "        output = self.bert_model(src, token_type_input)\n",
        "        # transpose output (S, N, E) to (N, S, E)\n",
        "        output = output.transpose(0, 1)\n",
        "        output = self.activation(output)\n",
        "        pos_output = self.qa_span(output)\n",
        "        start_pos, end_pos = pos_output.split(1, dim=-1)\n",
        "        start_pos = start_pos.squeeze(-1)\n",
        "        end_pos = end_pos.squeeze(-1)\n",
        "        return start_pos, end_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9wR51SxKRp_",
        "colab_type": "text"
      },
      "source": [
        " The pretrained model is available here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F6QIf30Id8p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3012bc3e-d38c-4436-d121-23a1ab6c22a4"
      },
      "source": [
        "%%shell\n",
        "rm *.pt\n",
        "wget https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/full_ns_bert.pt\n",
        "md5sum -c <<<\"8070efa65373e0fb28ed23e1861c0def full_ns_bert.pt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-22 18:20:14--  https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/full_ns_bert.pt\n",
            "Resolving pytorch.s3.amazonaws.com (pytorch.s3.amazonaws.com)... 52.216.184.11\n",
            "Connecting to pytorch.s3.amazonaws.com (pytorch.s3.amazonaws.com)|52.216.184.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 660517596 (630M) [binary/octet-stream]\n",
            "Saving to: ‘full_ns_bert.pt’\n",
            "\n",
            "full_ns_bert.pt     100%[===================>] 629.92M  34.6MB/s    in 19s     \n",
            "\n",
            "2020-09-22 18:20:33 (33.7 MB/s) - ‘full_ns_bert.pt’ saved [660517596/660517596]\n",
            "\n",
            "full_ns_bert.pt: OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jMwRkP1InFU",
        "colab_type": "text"
      },
      "source": [
        "and can be loaded to the question answer task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1vNeEkQIH1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_bert = torch.load('full_ns_bert.pt')\n",
        "qa_model = QuestionAnswerTask(pretrained_bert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV9ylnHCIRTh",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Fine-tuning the Model\n",
        "\n",
        "Then, we fine-tune the BERT model with the question-answer task based on the SQuAD1 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMa0bgCUIWNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def fine_tune(model, optimizer, criterion, batch_size, device):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True,\n",
        "                            collate_fn=collate_batch)\n",
        "    for idx, (seq_input, ans_pos, tok_type) in enumerate(dataloader):\n",
        "        seq_input = seq_input.t().contiguous().to(device)\n",
        "        tok_type = tok_type.t().contiguous().to(device)\n",
        "        optimizer.zero_grad()\n",
        "        start_pos, end_pos = model(seq_input, token_type_input=tok_type)\n",
        "        target_start_pos, target_end_pos = ans_pos[0].to(device).split(1, dim=-1)\n",
        "        target_start_pos = target_start_pos.squeeze(-1)\n",
        "        target_end_pos = target_end_pos.squeeze(-1)\n",
        "        loss = (criterion(start_pos, target_start_pos) + criterion(end_pos, target_end_pos)) / 2\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | '\n",
        "                  'ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(epoch, idx,\n",
        "                                                      len(train_dataset) // batch_size,\n",
        "                                                      scheduler.get_last_lr()[0],\n",
        "                                                      elapsed * 1000 / log_interval,\n",
        "                                                      cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "            \n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 0.5  # learning rate\n",
        "BATCH_SIZE = 72 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(qa_model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    fine_tune(qa_model, optimizer, criterion, BATCH_SIZE, device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}