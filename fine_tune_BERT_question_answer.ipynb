{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tune_BERT_question_answer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8VBxFwPzfjrhgoghjnq+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangguanheng66/tutorials/blob/bert_question_answer/fine_tune_BERT_question_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVptaygFi5dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "rm -r /usr/local/lib/python3.6/dist-packages/torch*\n",
        "pip install numpy\n",
        "pip install --pre torch torchvision torchtext -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrIJHOEyO7su",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext.experimental.transforms import basic_english_normalize\n",
        "tokenizer = basic_english_normalize()\n",
        "tokenizer('here is an example')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AZuMstAGT1m",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning BERT model for Question-Answer with Torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6SWfPD4Gbpr",
        "colab_type": "text"
      },
      "source": [
        "Recently, we release a new torchtext library to address a few issues from OSS community ([torchtext 0.7.0 release note](https://github.com/pytorch/text/releases/tag/v0.7.0-rc3) and 0.8.0 release note). To accelerate research, the new torchtext library will provide reusable, orthogonal, correct, and performant building blocks  (Vectors, Vocab, Tokenizer) for cutting-edge experimentation based on deep knowledge of the NLP research and communities. In addition, we deeply integrate torchtext with broad range of PyTorch capabilities, such as Just-in-Time (JIT), quantization, distributed, and mobile, to enable seamless research-to-production for core end-to-end applications.\n",
        "\n",
        "In this tutorial, we are going to fune-tune a pretrained BERT model for question-answer task with the new library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DylRHo85Gw0F",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Prepare datasets\n",
        "\n",
        "We have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer backed by regular expression, and sentencepiece. Those are the basic data processing building blocks for raw text string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "splH7rFPG-En",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizer-vocabulary data processing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjLe1KKbreLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare data pipeline for question-answer dataset\n",
        "from torchtext.experimental.transforms import (\n",
        "    basic_english_normalize,\n",
        "    TextSequentialTransforms,\n",
        "    ToLongTensor,\n",
        ")\n",
        "from torchtext.experimental.vocab import vocab_from_file\n",
        "vocab = vocab_from_file(open('bert_vocab.txt', 'r'))\n",
        "text_pipeline = TextSequentialTransforms(basic_english_normalize(), vocab)\n",
        "pos_pipeline = ToLongTensor() # work on the position ids of the answer\n",
        "qa_data_pipeline= {'context': text_pipeline, 'question': text_pipeline,\n",
        "                   'answers': text_pipeline, 'ans_pos': pos_pipeline}\n",
        "\n",
        "# Check out raw text\n",
        "from torchtext.experimental.datasets.raw import SQuAD1\n",
        "train, dev = SQuAD1()\n",
        "            \n",
        "# Use the question-answer dataset template\n",
        "from torchtext.experimental.datasets.question_answer import QuestionAnswerDataset\n",
        "train_data = QuestionAnswerDataset([item for item in train], vocab, qa_data_pipeline)\n",
        "dev_data = QuestionAnswerDataset([item for item in dev], vocab, qa_data_pipeline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKXuIDHaHNss",
        "colab_type": "text"
      },
      "source": [
        "### Word-vector embedding data processing pipeline\n",
        "\n",
        "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. FastText and GloVe are well established baseline word vectors in the NLP community. In the new torchtext library, a Vector object supports the mapping between tokens and their corresponding vector representation (i.e. word embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSillz6wHTDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    basic_english_normalize,\n",
        "    TextSequentialTransforms,\n",
        ")\n",
        "from torchtext.experimental.vectors import FastText\n",
        "vector = FastText()\n",
        "word_vector_pipeline = TextSequentialTransforms(basic_english_normalize(), vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITgbca8OHY-p",
        "colab_type": "text"
      },
      "source": [
        "### SentencePiece data processing pipeline\n",
        "\n",
        "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. For sentencepiece transforms in torchtext, both subword units (e.g., byte-pair-encoding (BPE) ) and unigram language model are supported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNHW2FxwHfeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    pretrained_spm,\n",
        "    SentencePieceTransform,\n",
        ")\n",
        "spm = pretrained_spm()\n",
        "spm_transform = SentencePieceTransform(spm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKyPu6nfHigd",
        "colab_type": "text"
      },
      "source": [
        "### JIT support for the data processing pipeline\n",
        "\n",
        "The new building blocks in torchtext library is compatible with torch.jit.script. TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency. The data processing pipelines above can be converted and run on the JIT mode without Python dependency\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO6FWVGnHnYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "jit_text_pipeline = torch.jit.script(text_pipeline.to_ivalue())\n",
        "jit_pos_pipeline = torch.jit.script(pos_pipeline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmqHKb28HlS3",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Data Iterator\n",
        "\n",
        "The PyTorch data loading utility is the [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class. It works with a map-style dataset that implements the __getitem__() and __len__() protocols, and represents a map from indices/keys to data sampl*e*s. Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgcfY4fhH-Qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# [TODO]\n",
        "# Need to integrate with torchtext.experimental.transforms.PadTransform\n",
        "def collate_batch(batch):\n",
        "    seq_list, ans_pos_list, tok_type = [], [], []\n",
        "    for item in batch:\n",
        "        qa_item = torch.cat((torch.tensor([cls_id]), item['question'], torch.tensor([sep_id]),\n",
        "                             item['context'], torch.tensor([sep_id])))\n",
        "        seq_list.append(qa_item)\n",
        "        pos_list = [pos + item['question'].size(0) + 2 for pos in item['ans_pos']]\n",
        "        ans_pos_list.append(pos_list)\n",
        "        tok_type.append(torch.cat((torch.zeros((item['question'].size(0) + 2)),\n",
        "                                   torch.ones((args.bptt - item['question'].size(0) - 2)))))\n",
        "    _ans_pos_list = []\n",
        "    for pos in zip(*ans_pos_list):\n",
        "        _ans_pos_list.append(torch.stack(list(pos)))\n",
        "    return torch.stack(seq_list).long().t().contiguous().to(device), \\\n",
        "        _ans_pos_list, torch.stack(tok_type).long().t().contiguous().to(device)\n",
        "        \n",
        "dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH0aoiGxH0P4",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Model for Question-Answer Task\n",
        "\n",
        "A BERT model was pretrained with the masked language modeling task and next-sentence task according to the paper - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).  torchtext.datasets.WikiText103 and BookCorpus were used to pre-trained the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-_8lstuIMci",
        "colab_type": "text"
      },
      "source": [
        "Here is the layout of the model for the question-answer task. On the top of the BERT model, there is a linear layer to project the position of the answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVMQBuOFIO7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "class QuestionAnswerTask(torch.nn.Module):\n",
        "    \"\"\"Contain a pretrain BERT model and a linear layer.\"\"\"\n",
        "\n",
        "    def __init__(self, bert_model):\n",
        "        super(QuestionAnswerTask, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.activation = nn.functional.gelu\n",
        "        self.qa_span = nn.Linear(bert_model.ninp, 2)\n",
        "\n",
        "    def forward(self, src, token_type_input):\n",
        "        output = self.bert_model(src, token_type_input)\n",
        "        # transpose output (S, N, E) to (N, S, E)\n",
        "        output = output.transpose(0, 1)\n",
        "        output = self.activation(output)\n",
        "        pos_output = self.qa_span(output)\n",
        "        start_pos, end_pos = pos_output.split(1, dim=-1)\n",
        "        start_pos = start_pos.squeeze(-1)\n",
        "        end_pos = end_pos.squeeze(-1)\n",
        "        return start_pos, end_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9wR51SxKRp_",
        "colab_type": "text"
      },
      "source": [
        " The pretrained model is available here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F6QIf30Id8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "wget https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/full_ns_bert.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jMwRkP1InFU",
        "colab_type": "text"
      },
      "source": [
        "and can be loaded to the question answer task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1vNeEkQIH1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_bert = torch.load('full_ns_bert.pt')\n",
        "qa_model = QuestionAnswerTask(pretrained_bert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV9ylnHCIRTh",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Fine-tuning the Model\n",
        "\n",
        "Then, we fine-tune the BERT model with the question-answer task based on the SQuAD1 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMa0bgCUIWNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fine_tune(model, optimizer, criterion, batch_size):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True,\n",
        "                            collate_fn=collate_batch)\n",
        "    for idx, (seq_input, ans_pos, tok_type) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        start_pos, end_pos = model(seq_input, token_type_input=tok_type)\n",
        "        target_start_pos, target_end_pos = ans_pos[0].to(device).split(1, dim=-1)\n",
        "        target_start_pos = target_start_pos.squeeze(-1)\n",
        "        target_end_pos = target_end_pos.squeeze(-1)\n",
        "        loss = (criterion(start_pos, target_start_pos) + criterion(end_pos, target_end_pos)) / 2\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            train_loss_log[-1] = cur_loss\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | '\n",
        "                  'ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(epoch, idx,\n",
        "                                                      len(train_dataset) // batch_size,\n",
        "                                                      scheduler.get_last_lr()[0],\n",
        "                                                      elapsed * 1000 / log_interval,\n",
        "                                                      cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "            \n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 0.5  # learning rate\n",
        "BATCH_SIZE = 72 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(qa_model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    fine_tune(qa_model, optimizer, criterion, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}