{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torchtext with text classification analysis",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangguanheng66/tutorials/blob/sentiment_analysis/Torchtext_with_text_classification_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMg-_Db4xil_"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# pip uninstall torch torchtext\n",
        "rm -r /usr/local/lib/python3.6/dist-packages/torch*\n",
        "pip install --pre torch torchtext -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n",
        "# pip install --pre torch==1.8.0.dev20201008+cu101 torchtext==0.8.0.dev20201008 -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjjwtY1Exr-e"
      },
      "source": [
        "import torch\n",
        "import torchtext"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIBfeik3xzT-"
      },
      "source": [
        "# Prototype pipeline with the new torchtext library\n",
        "\n",
        "In this tutorial, we will show how to use the new torchtext library to build the dataset for the text classification analysis. In the nightly release of torchtext libraries, we provide a few prototype building blocks for data processing. With the new torchtext library, you will have the flexibility to\n",
        "\n",
        "*   Access to the raw data as an iterator\n",
        "*   Build data processing pipeline to convert the raw text strings into `torch.Tensor` that can be used to train the model\n",
        "*   Shuffle and iterate the data with [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzujHlKmQvr1"
      },
      "source": [
        "## Step 1: Access to the raw dataset iterators\n",
        "----------------------------\n",
        "\n",
        "For some advanced users, they prefer to work on the raw data strings with their custom data process pipeline. The new torchtext library provides a few raw dataset iterators, which yield the raw text strings. For example, the AG_NEWS dataset iterators yield the raw data as a tuple of label and text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTTFwHimQunR"
      },
      "source": [
        "from torchtext.experimental.datasets.raw import AG_NEWS\n",
        "train_iter, = AG_NEWS(data_select=('train'))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb0APgAhRO-P"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "next(iter(train_iter))\n",
        ">>> (3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - \n",
        "Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green \n",
        "again.\")\n",
        "\n",
        "next(iter(train_iter))\n",
        ">>> (3, 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private \n",
        "investment firm Carlyle Group,\\\\which has a reputation for making well-timed \n",
        "and occasionally\\\\controversial plays in the defense industry, has quietly \n",
        "placed\\\\its bets on another part of the market.')\n",
        "\n",
        "next(iter(train_iter))\n",
        ">>> (3, \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring \n",
        "crude prices plus worries\\\\about the economy and the outlook for earnings are \n",
        "expected to\\\\hang over the stock market next week during the depth of \n",
        "the\\\\summer doldrums.\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0hKv5a9yBh2"
      },
      "source": [
        "## Step 2: Prepare data processing pipelines\n",
        "----------------------------\n",
        "We have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer backed by regular expression, and sentencepiece. Those are the basic data processing building blocks for raw text string.\n",
        "\n",
        "### 2.1 Tokenizer-vocabulary data processing pipeline\n",
        "\n",
        "Here is an example for typical NLP data processing with tokenizer and vocabulary.\n",
        "\n",
        "The first step is to build a vocabulary with the raw training dataset. We provide a function `build_vocab_from_iterator` to build the vocabulary from a text iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ8e0CPfNSBU",
        "outputId": "dbd5d633-fc81-47af-f758-12b718ad4875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from torchtext.experimental.vocab import build_vocab_from_iterator\n",
        "from torchtext.experimental.transforms import basic_english_normalize\n",
        "tokenizer = basic_english_normalize()\n",
        "train_iter, = AG_NEWS(data_select=('train',))\n",
        "vocab = build_vocab_from_iterator(iter(tokenizer(line) for label, line in train_iter))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/experimental/vocab.py:136: RuntimeWarning: The `unk_token` '<unk>' wasn't found in the `ordered_dict`. Adding the `unk_token` to the beginning of the Vocab.\n",
            "  \"to the beginning of the Vocab.\".format(unk_token), RuntimeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skoJqORdQDdM"
      },
      "source": [
        "The vocabulary block converts a list of tokens into integers.\n",
        "```\n",
        "vocab(['here', 'is', 'an', 'example'])\n",
        ">>> [475, 21, 30, 5286]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBhnX9u-ya6T"
      },
      "source": [
        "Prepare data pipeline with the tokenizer and vocabulary. The pipelines will be used for the raw data strings from the dataset iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC4gaPr5ybzQ"
      },
      "source": [
        "def generate_text_pipeline(tokenizer, vocab):\n",
        "  def _forward(text):\n",
        "    return vocab(tokenizer(text))\n",
        "  return _forward\n",
        "text_pipeline = generate_text_pipeline(basic_english_normalize(), vocab)\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsZYFwWiUKJ_"
      },
      "source": [
        "The text pipeline converts a text string into a list of integers based on the lookup defined in the vocab. The label pipeline converts the label into integers. For example,\n",
        "\n",
        "```\n",
        "text_pipeline('here is the an example')\n",
        ">>> [475, 21, 2, 30, 5286]\n",
        "label_pipeline('10')\n",
        ">>> 9\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLNPAmG-zBtC"
      },
      "source": [
        "### 2.2 SentencePiece data processing pipeline\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. For sentencepiece transforms in torchtext, both subword units (e.g., byte-pair-encoding (BPE) ) and unigram language model are supported. We provide a few pretrained SentencePiece models and they are accessable from `PRETRAINED_SP_MODEL`. Here is an example to apply SentencePiece transform to build the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_EMgLIfzScN"
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    PRETRAINED_SP_MODEL,\n",
        "    sentencepiece_processor,\n",
        "    load_sp_model,\n",
        ")\n",
        "from torchtext.utils import download_from_url\n",
        "spm_filepath = download_from_url(PRETRAINED_SP_MODEL['text_unigram_25000'])\n",
        "spm_transform = sentencepiece_processor(spm_filepath)\n",
        "sp_model = load_sp_model(spm_filepath)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L42bQso4XUa"
      },
      "source": [
        "The sentecepiece processor converts a text string into a list of integers. You can use the `decode` method to convert a list of integers back to the original string.\n",
        "\n",
        "```\n",
        "spm_transform('here is the an example')\n",
        ">>> [130, 46, 9, 76, 1798]\n",
        "spm_transform.decode([6468, 17151, 4024, 8246, 16887, 87, 23985, 12, 581, 15120])\n",
        ">>> 'torchtext sentencepiece processor can encode and decode'\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXtEz6DJ2i7M"
      },
      "source": [
        "## Step 3: Generate data batch and iterator¶\n",
        "\n",
        "The PyTorch data loading utility is the [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) class. It works with a map-style dataset that implements the `getitem()` and `len()` protocols, and represents a map from indices/keys to data samples. It also works with an iterable datasets with the shuffle argumnet of `False`. \n",
        "\n",
        "Before sending to the model, `collate_fn` function works on a batch of samples generated from DataLoader. The input to `collat_fn` is a batch of data with the batch size in `DataLoader`, and `collate_fn` processes them according to the data processing pipelines declared on Step 2. Pay attention here and make sure that collate_fn is declared as a top level def. This ensures that the function is available in each worker. \n",
        "\n",
        "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of `nn.EmbeddingBag`. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of indidividual text entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvWw9sVE2idq"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(spm_transform(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)    \n",
        "\n",
        "train_iter, = AG_NEWS(data_select=('train'))\n",
        "dataloader = DataLoader(list(train_iter), batch_size=8, shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNbi1He69ANc"
      },
      "source": [
        "## Step 4: Model for text classification task\n",
        "---\n",
        "\n",
        "We use a simple model here for the text classification analysis. The model is composed of the [nn.EmbeddingBag](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#embeddingbag) layer plus a linear layer for the classification purpose. `nn.EmbeddingBag` computes the mean value of a “bag” of embeddings. Although the text entries here have different lengths, `nn.EmbeddingBag` module requires no padding here since the text lengths are saved in offsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLJYfMlq9HjU"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCCaM9x_6rIN"
      },
      "source": [
        "We build a model with the embedding dimension of 64. The AG_NEWS dataset has four labels and therefore the number of classes is four.\n",
        "\n",
        "*   1 : World\n",
        "*   2 : Sports\n",
        "*   3 : Business\n",
        "*   4 : Sci/Tec\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfyBKOxg_kg5"
      },
      "source": [
        "from torchtext.experimental.datasets.text_classification import LABELS\n",
        "num_class = len(LABELS['AG_NEWS'])\n",
        "vocab_size = sp_model.GetPieceSize()\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iovYWRYFBHnc"
      },
      "source": [
        "\n",
        "## Step 5: Train and test the model\n",
        "---\n",
        "\n",
        "Then, we train and test the model with the text classification datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngQ1bnGlBZ0k"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(model, dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | '\n",
        "                  'ms/batch {:5.2f} '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              scheduler.get_last_lr()[0],\n",
        "                                              elapsed * 1000 / log_interval,\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    ans_pred_tokens_samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSBskbHkQ3Ll"
      },
      "source": [
        "[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in a single class. It is useful when training a classification problem with C classes. [SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html) implements stochastic gradient descent method as optimizer. [StepLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR) is used here to adjust the learning rate through epochs.\n",
        "\n",
        "Here are a few hyperparameters used in the pipeline\n",
        "\n",
        "\n",
        "*   The number of epoches - 10\n",
        "*   The initial learning rate - 5.0\n",
        "*   The batch size - 64\n",
        "*   The maximum sequence length - 768\n",
        "\n",
        "Since the original AG_NEWS has no valid dataset, we split the training dataset into train/valid sets with a split ratio of 0.95 (train) and 0.05 (valid). Here we use [torch.utils.data.dataset.random_split](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split) function in PyTorch core library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXD1I31kCE2T",
        "outputId": "33acd605-911d-4bd1-d2d2-1c6b6d241789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = list(train_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(list(test_iter), batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model, train_dataloader)\n",
        "    accu_val = evaluate(model, valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 89)\n",
        "print('Checking the results of test dataset...')\n",
        "accu_test = evaluate(model, test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   500/ 1782 batches | lr 5.00000 | ms/batch 11.80 | accuracy    0.641\n",
            "| epoch   1 |  1000/ 1782 batches | lr 5.00000 | ms/batch 11.80 | accuracy    0.841\n",
            "| epoch   1 |  1500/ 1782 batches | lr 5.00000 | ms/batch 11.51 | accuracy    0.872\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 21.68s | valid accuracy    0.885 \n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   500/ 1782 batches | lr 5.00000 | ms/batch 12.23 | accuracy    0.893\n",
            "| epoch   2 |  1000/ 1782 batches | lr 5.00000 | ms/batch 12.09 | accuracy    0.897\n",
            "| epoch   2 |  1500/ 1782 batches | lr 5.00000 | ms/batch 11.43 | accuracy    0.901\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 22.09s | valid accuracy    0.904 \n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   500/ 1782 batches | lr 5.00000 | ms/batch 11.89 | accuracy    0.910\n",
            "| epoch   3 |  1000/ 1782 batches | lr 5.00000 | ms/batch 11.86 | accuracy    0.912\n",
            "| epoch   3 |  1500/ 1782 batches | lr 5.00000 | ms/batch 11.59 | accuracy    0.911\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 21.83s | valid accuracy    0.906 \n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   500/ 1782 batches | lr 5.00000 | ms/batch 11.79 | accuracy    0.920\n",
            "| epoch   4 |  1000/ 1782 batches | lr 5.00000 | ms/batch 11.75 | accuracy    0.918\n",
            "| epoch   4 |  1500/ 1782 batches | lr 5.00000 | ms/batch 11.64 | accuracy    0.920\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 21.66s | valid accuracy    0.906 \n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   500/ 1782 batches | lr 5.00000 | ms/batch 11.85 | accuracy    0.925\n",
            "| epoch   5 |  1000/ 1782 batches | lr 5.00000 | ms/batch 11.60 | accuracy    0.925\n",
            "| epoch   5 |  1500/ 1782 batches | lr 5.00000 | ms/batch 11.52 | accuracy    0.923\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 21.74s | valid accuracy    0.913 \n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   500/ 1782 batches | lr 5.00000 | ms/batch 11.53 | accuracy    0.932\n",
            "| epoch   6 |  1000/ 1782 batches | lr 5.00000 | ms/batch 11.82 | accuracy    0.929\n",
            "| epoch   6 |  1500/ 1782 batches | lr 5.00000 | ms/batch 11.80 | accuracy    0.929\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 21.60s | valid accuracy    0.908 \n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   500/ 1782 batches | lr 0.50000 | ms/batch 11.68 | accuracy    0.942\n",
            "| epoch   7 |  1000/ 1782 batches | lr 0.50000 | ms/batch 11.86 | accuracy    0.943\n",
            "| epoch   7 |  1500/ 1782 batches | lr 0.50000 | ms/batch 11.46 | accuracy    0.941\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 21.70s | valid accuracy    0.915 \n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   500/ 1782 batches | lr 0.50000 | ms/batch 11.68 | accuracy    0.946\n",
            "| epoch   8 |  1000/ 1782 batches | lr 0.50000 | ms/batch 11.71 | accuracy    0.944\n",
            "| epoch   8 |  1500/ 1782 batches | lr 0.50000 | ms/batch 11.41 | accuracy    0.940\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 21.52s | valid accuracy    0.914 \n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   500/ 1782 batches | lr 0.05000 | ms/batch 11.65 | accuracy    0.942\n",
            "| epoch   9 |  1000/ 1782 batches | lr 0.05000 | ms/batch 11.99 | accuracy    0.947\n",
            "| epoch   9 |  1500/ 1782 batches | lr 0.05000 | ms/batch 11.59 | accuracy    0.945\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 21.78s | valid accuracy    0.916 \n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   500/ 1782 batches | lr 0.05000 | ms/batch 12.14 | accuracy    0.946\n",
            "| epoch  10 |  1000/ 1782 batches | lr 0.05000 | ms/batch 11.49 | accuracy    0.946\n",
            "| epoch  10 |  1500/ 1782 batches | lr 0.05000 | ms/batch 11.55 | accuracy    0.944\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 21.66s | valid accuracy    0.915 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Checking the results of test dataset...\n",
            "test accuracy    0.912\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}