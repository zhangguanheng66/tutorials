{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torchtext with sentiment analysis",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNb8rGJlHQ4YARmhjgB089P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangguanheng66/tutorials/blob/sentiment_analysis/Torchtext_with_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMg-_Db4xil_"
      },
      "source": [
        "%%shell\n",
        "\n",
        "rm -r /usr/local/lib/python3.6/dist-packages/torch*\n",
        "pip install numpy\n",
        "pip install --pre torch torchtext -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjjwtY1Exr-e"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "print(torchtext.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIBfeik3xzT-"
      },
      "source": [
        "## Sentiment Analysis with Torchtext\n",
        "\n",
        "This tutorial is to show how to build the dataset to conduct sentiment analysis with torchtext library. The builiding blocks in torchtext library give the flexibility to build a custom data processing pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0hKv5a9yBh2"
      },
      "source": [
        "## Step 1: Prepare datasets\n",
        "----------------------------\n",
        "We have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer backed by regular expression, and sentencepiece. Those are the basic data processing building blocks for raw text string.\n",
        "\n",
        "### Tokenizer-vocabulary data processing pipeline\n",
        "\n",
        "Here is an example for typical NLP data processing with tokenizer and vocabulary. We have a vocabulary saved in a text file. It's avaiable for downloading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJFgmzsAyYWH"
      },
      "source": [
        "%%shell\n",
        "rm bert_vocab.txt\n",
        "wget https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/bert_vocab.txt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBhnX9u-ya6T"
      },
      "source": [
        "Prepare data pipeline for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC4gaPr5ybzQ"
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    basic_english_normalize,\n",
        "    TextSequentialTransforms,\n",
        ")\n",
        "from torchtext.experimental.vocab import vocab_from_file\n",
        "with open('bert_vocab.txt', 'r') as f:\n",
        "  vocab = vocab_from_file(f)\n",
        "text_pipeline = TextSequentialTransforms(basic_english_normalize(), vocab)\n",
        "label_pipeline = lambda x: 1 if x == 'pos' else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PllfOE8-y7ye"
      },
      "source": [
        "### (Optional for tutorial) Word-vector embedding data processing pipeline\n",
        "\n",
        "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. FastText and GloVe are well established baseline word vectors in the NLP community. In the new torchtext library, a Vector object supports the mapping between tokens and their corresponding vector representation (i.e. word embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE7D5dmdy89K"
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    basic_english_normalize,\n",
        "    TextSequentialTransforms,\n",
        ")\n",
        "from torchtext.experimental.vectors import FastText\n",
        "vector = FastText()\n",
        "word_vector_pipeline = TextSequentialTransforms(basic_english_normalize(), vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLNPAmG-zBtC"
      },
      "source": [
        "### (Optional for tutorial) SentencePiece data processing pipeline\n",
        "\n",
        "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. For sentencepiece transforms in torchtext, both subword units (e.g., byte-pair-encoding (BPE) ) and unigram language model are supported. Here is an example to apply SentencePiece transform to build the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_EMgLIfzScN"
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    PRETRAINED_SP_MODEL,\n",
        "    sentencepiece_processor,\n",
        ")\n",
        "from torchtext.utils import download_from_url\n",
        "spm_filepath = download_from_url(PRETRAINED_SP_MODEL['text_unigram_25000'])\n",
        "spm_transform = sentencepiece_processor(spm_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXZKwuNt0glh"
      },
      "source": [
        "### Construct the dataset with raw text iterator and transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Tabbs8yjcS"
      },
      "source": [
        "The raw text datasets iterators are avaialble in the `torchtext.experimental.datasets.raw` folder. The datasets in `torchtext.experimental.datasets.raw` return iterators which yield the raw data. In this way, users can define the custom data processing pipelines and work on the raw data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWPxR8sMykjI"
      },
      "source": [
        "from torchtext.experimental.datasets.raw import IMDB\n",
        "train_iter, test_iter = IMDB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHbI3Xbpyqbr"
      },
      "source": [
        "Materialize the raw IMDB data iterators. Pass the data and data processing pipelines (a.k.a. transforms) to the IMDB dataset abstraction. IMDBDataset is an abstraction that applies the user-defined transform pipelines to the raw question-answer data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnuE5em1yv2T"
      },
      "source": [
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Defines an abstract datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, transforms):\n",
        "        \"\"\"Initiate text-classification dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        super(IMDBDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.transforms = transforms  # (label_transforms, text_transforms)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        label = self.data[i][0]\n",
        "        txt = self.data[i][1]\n",
        "        return (self.transforms[0](label), self.transforms[1](txt))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "train_data = IMDBDataset(list(train_iter), (label_pipeline, spm_transform))\n",
        "test_data = IMDBDataset(list(test_iter), (label_pipeline, spm_transform))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stbFHx8K2X-2"
      },
      "source": [
        "\n",
        "### [REMOVE LATER] JIT support for the data processing pipeline\n",
        "\n",
        "The new building blocks in torchtext library is compatible with `torch.jit.script`. TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency. The data processing pipelines above can be converted and run on the JIT mode without Python dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY873g3Q2bjH"
      },
      "source": [
        "text_pipeline = text_pipeline.to_ivalue()\n",
        "jit_text_pipeline = torch.jit.script(text_pipeline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXtEz6DJ2i7M"
      },
      "source": [
        "## Step 2: Data IteratorÂ¶\n",
        "\n",
        "The PyTorch data loading utility is the `torch.utils.data.DataLoader` class. It works with a map-style dataset that implements the `getitem()` and `len()` protocols, and represents a map from indices/keys to data samples. Before sending to the model, `collate_fn` function works on a batch of samples generated from DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvWw9sVE2idq"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# [TODO] integrate with torchtext.experimental.transforms.PadTransform\n",
        "# Need to land https://github.com/pytorch/text/pull/952\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "cls_id = spm_transform.sp_model.PieceToId('<cls>')\n",
        "pad_id = spm_transform.sp_model.PieceToId('<pad>')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(_label)\n",
        "        text_list.append(torch.tensor(cls_id + _text))\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=float(pad_id[0]))\n",
        "    label_list = torch.tensor(label_list)\n",
        "    return label_list.to(device), text_list.transpose(0, 1).contiguous().to(device)\n",
        "\n",
        "dataloader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNbi1He69ANc"
      },
      "source": [
        "## Step 3: Model for Sentiment Analysis Task\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLJYfMlq9HjU"
      },
      "source": [
        "from torch import nn\n",
        "class SentimentAnalysisModel(nn.Module):\n",
        "    \"\"\"Contain a transformer encoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(SentimentAnalysisModel, self).__init__()\n",
        "        self.embed_layer = nn.Embedding(ntoken, ninp)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.projection = nn.Linear(ninp, 2)\n",
        "\n",
        "    def forward(self, src_seq):\n",
        "        output = self.embed_layer(src_seq)\n",
        "        output = self.transformer_encoder(output)\n",
        "        output = self.activation(output[0])\n",
        "        return self.projection(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfyBKOxg_kg5"
      },
      "source": [
        "vocab_size = spm_transform.sp_model.GetPieceSize()\n",
        "emsize, nhead, nhid, nlayers, dropout = 64, 8, 128, 1, 0.2\n",
        "model = SentimentAnalysisModel(vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iovYWRYFBHnc"
      },
      "source": [
        "\n",
        "## Step 4: Train and test the Model\n",
        "---\n",
        "\n",
        "Then, we train and test the transformer model with the sentiment analysis based on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngQ1bnGlBZ0k"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def fine_tune(model, dataloader, optimizer, criterion, batch_size, device, SEQENCE_LENGTH):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        # print(seq_input.size(), tok_type.size())\n",
        "        if text.size(0) > SEQENCE_LENGTH:\n",
        "            text = text[:SEQENCE_LENGTH]\n",
        "        predited_label = model(text)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | '\n",
        "                  'ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(epoch, idx, len(dataloader),\n",
        "                                                      scheduler.get_last_lr()[0],\n",
        "                                                      elapsed * 1000 / log_interval,\n",
        "                                                      cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model, dataloader, optimizer, criterion, batch_size, device, SEQENCE_LENGTH):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ans_pred_tokens_samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            if text.size(0) > SEQENCE_LENGTH:\n",
        "              text = text[:SEQENCE_LENGTH]\n",
        "            predited_label = model(text)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSBskbHkQ3Ll"
      },
      "source": [
        "Here are a few hyperparameters used in the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXD1I31kCE2T"
      },
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 3 # epoch\n",
        "LR = 0.5  # learning rate\n",
        "BATCH_SIZE = 16 # batch size for training\n",
        "SEQENCE_LENGTH = 128 # the maximum sequence length\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "eval_loss = None\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    fine_tune(model, train_dataloader, optimizer, criterion, BATCH_SIZE, device, SEQENCE_LENGTH)\n",
        "    _loss = evaluate(model, test_dataloader, optimizer, criterion, BATCH_SIZE, device, SEQENCE_LENGTH)\n",
        "    if eval_loss is not None and _loss > eval_loss:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       eval_loss = _loss\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid loss {:5.2f} | '.format(epoch, (time.time() - epoch_start_time),\n",
        "                                         _loss))\n",
        "    print('-' * 89)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}