{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torchtext with sentiment analysis",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOssNU1HZabSDterIv5QkWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangguanheng66/tutorials/blob/sentiment_analysis/Torchtext_with_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMg-_Db4xil_",
        "outputId": "8fa58731-81cc-4b08-baab-a015da3bb3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "rm -r /usr/local/lib/python3.6/dist-packages/torch*\n",
        "pip install numpy\n",
        "pip install --pre torch torchtext -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n",
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/nightly/cu101/torch-1.7.0.dev20201001%2Bcu101-cp36-cp36m-linux_x86_64.whl (735.3MB)\n",
            "\u001b[K     |████████████████████████████████| 735.3MB 24kB/s \n",
            "\u001b[?25hCollecting torchtext\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/nightly/torchtext-0.8.0.dev20201002-cp36-cp36m-linux_x86_64.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 568kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch, sentencepiece, torchtext\n",
            "Successfully installed sentencepiece-0.1.91 torch-1.7.0.dev20201001+cu101 torchtext-0.8.0.dev20201002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjjwtY1Exr-e",
        "outputId": "32024a9b-4fc0-4f8a-9f99-7fc98157e3ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "print(torchtext.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-970b4ef6dba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transforms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/experimental/transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexTokenizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mRegexTokenizerPybind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.6/dist-packages/torchtext/_torchtext.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceIN3c108quint4x2EEEPKNS_6detail12TypeMetaDataEv",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIBfeik3xzT-"
      },
      "source": [
        "## Sentiment Analysis with Torchtext\n",
        "\n",
        "This tutorial is to show how to conduct sentiment analysis with torchtext library\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0hKv5a9yBh2"
      },
      "source": [
        "## Step 1: Prepare datasets\n",
        "----------------------------\n",
        "We have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer backed by regular expression, and sentencepiece. Those are the basic data processing building blocks for raw text string.\n",
        "\n",
        "### Tokenizer-vocabulary data processing pipeline\n",
        "\n",
        "Download a vocab text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJFgmzsAyYWH"
      },
      "source": [
        "%%shell\n",
        "rm bert_vocab.txt\n",
        "wget https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/bert_vocab.txt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBhnX9u-ya6T"
      },
      "source": [
        "Prepare data pipeline for the question-answer dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC4gaPr5ybzQ"
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    basic_english_normalize,\n",
        "    TextSequentialTransforms,\n",
        ")\n",
        "from torchtext.experimental.vocab import vocab_from_file\n",
        "with open('bert_vocab.txt', 'r') as f:\n",
        "  vocab = vocab_from_file(f)\n",
        "text_pipeline = TextSequentialTransforms(basic_english_normalize(), vocab)\n",
        "label_pipeline = lambda x: 1 if x == 'pos' else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PllfOE8-y7ye"
      },
      "source": [
        "### (Optional for tutorial) Word-vector embedding data processing pipeline\n",
        "\n",
        "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. FastText and GloVe are well established baseline word vectors in the NLP community. In the new torchtext library, a Vector object supports the mapping between tokens and their corresponding vector representation (i.e. word embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE7D5dmdy89K"
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    basic_english_normalize,\n",
        "    TextSequentialTransforms,\n",
        ")\n",
        "from torchtext.experimental.vectors import FastText\n",
        "vector = FastText()\n",
        "word_vector_pipeline = TextSequentialTransforms(basic_english_normalize(), vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLNPAmG-zBtC"
      },
      "source": [
        "### (Optional for tutorial) SentencePiece data processing pipeline\n",
        "\n",
        "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. For sentencepiece transforms in torchtext, both subword units (e.g., byte-pair-encoding (BPE) ) and unigram language model are supported.\n",
        "\n",
        "Here is an example to apply SentencePiece transform to build a Language Modeling dataset. Although the pretrained BERT model was generated on a different vocabulary, the follow LM dataset with the SentencePiece transform can be used to train a masked language model task (described in the BERT paper) from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_EMgLIfzScN"
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    PRETRAINED_SP_MODEL,\n",
        "    sentencepiece_processor,\n",
        ")\n",
        "from torchtext.utils import download_from_url\n",
        "spm_filepath = download_from_url(PRETRAINED_SP_MODEL['text_unigram_25000'])\n",
        "spm_transform = sentencepiece_processor(spm_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Tabbs8yjcS"
      },
      "source": [
        "The dataset in `torchtext.experimental.datasets.raw` returns iterators which yield the raw data. In this way, users can definte the custom data processing pipelines and work on the raw data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXZKwuNt0glh"
      },
      "source": [
        "### Construct the dataset with raw text iterator and transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppQ4G3b80x_g"
      },
      "source": [
        "The raw text datasets iterators are avaialble in the `torchtext.experimental.datasets.raw` folder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWPxR8sMykjI"
      },
      "source": [
        "from torchtext.experimental.datasets.raw import IMDB\n",
        "train_iter, test_iter = IMDB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHbI3Xbpyqbr"
      },
      "source": [
        "Materialize the raw IMDB data iterators. Pass the data and data processing pipelines (a.k.a. transforms) to the IMDB dataset abstraction. IMDBDataset is an abstraction (link) that applies the user-defined transform pipelines to the raw question-answer data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnuE5em1yv2T"
      },
      "source": [
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Defines an abstract datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, transforms):\n",
        "        \"\"\"Initiate text-classification dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        super(IMDBDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.transforms = transforms  # (label_transforms, text_transforms)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        label = self.data[i][0]\n",
        "        txt = self.data[i][1]\n",
        "        return (self.transforms[0](label), self.transforms[1](txt))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "train_data = IMDBDataset(list(train_iter), (label_pipeline, spm_transform))\n",
        "test_data = IMDBDataset(list(test_iter), (label_pipeline, spm_transform))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvlDS5wg1MyG"
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stbFHx8K2X-2"
      },
      "source": [
        "\n",
        "### JIT support for the data processing pipeline\n",
        "\n",
        "The new building blocks in torchtext library is compatible with torch.jit.script. TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency. The data processing pipelines above can be converted and run on the JIT mode without Python dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY873g3Q2bjH"
      },
      "source": [
        "text_pipeline = text_pipeline.to_ivalue()\n",
        "jit_text_pipeline = torch.jit.script(text_pipeline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXtEz6DJ2i7M"
      },
      "source": [
        "## Step 2: Data Iterator¶\n",
        "\n",
        "The PyTorch data loading utility is the `torch.utils.data.DataLoader` class. It works with a map-style dataset that implements the `getitem()` and `len()` protocols, and represents a map from indices/keys to data samples. Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvWw9sVE2idq"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# [TODO] integrate with torchtext.experimental.transforms.PadTransform\n",
        "# Need to land https://github.com/pytorch/text/pull/952\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "cls_id = vocab(['<cls>'])\n",
        "pad_id = vocab(['<pad>'])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(_label)\n",
        "        text_list.append(torch.tensor(cls_id + _text))\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=float(pad_id[0]))\n",
        "    label_list = torch.tensor(label_list)\n",
        "    return label_list.to(device), text_list.transpose(0, 1).contiguous().to(device)\n",
        "\n",
        "dataloader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNbi1He69ANc"
      },
      "source": [
        "## Step 3: Model for Sentiment Analysis Task\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLJYfMlq9HjU"
      },
      "source": [
        "from torch import nn\n",
        "class SentimentAnalysisModel(nn.Module):\n",
        "    \"\"\"Contain a transformer encoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(SentimentAnalysisModel, self).__init__()\n",
        "        self.embed_layer = nn.Embedding(ntoken, ninp)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.projection = nn.Linear(ninp, 2)\n",
        "\n",
        "    def forward(self, src_seq):\n",
        "        output = self.embed_layer(src_seq)\n",
        "        output = self.transformer_encoder(output)\n",
        "        output = self.activation(output[0])\n",
        "        return self.projection(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfyBKOxg_kg5"
      },
      "source": [
        "vocab_size, emsize, nhead, nhid, nlayers, dropout = 25000, 64, 8, 128, 1, 0.2\n",
        "model = SentimentAnalysisModel(vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iovYWRYFBHnc"
      },
      "source": [
        "\n",
        "## Step 4: Train and test the Model\n",
        "---\n",
        "\n",
        "Then, we train and test the transformer model with the sentiment analysis based on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngQ1bnGlBZ0k"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def fine_tune(model, dataloader, optimizer, criterion, batch_size, device, SEQENCE_LENGTH):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    log_interval = 20\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        # print(seq_input.size(), tok_type.size())\n",
        "        if text.size(0) > SEQENCE_LENGTH:\n",
        "            text = text[:SEQENCE_LENGTH]\n",
        "        predited_label = model(text)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | '\n",
        "                  'ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(epoch, idx, len(dataloader),\n",
        "                                                      scheduler.get_last_lr()[0],\n",
        "                                                      elapsed * 1000 / log_interval,\n",
        "                                                      cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model, dataloader, optimizer, criterion, batch_size, device, SEQENCE_LENGTH):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ans_pred_tokens_samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            if text.size(0) > SEQENCE_LENGTH:\n",
        "              text = text[:SEQENCE_LENGTH]\n",
        "            predited_label = model(text)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXD1I31kCE2T"
      },
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 3 # epoch\n",
        "LR = 0.5  # learning rate\n",
        "BATCH_SIZE = 16 # batch size for training\n",
        "SEQENCE_LENGTH = 128 # the maximum sequence length\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "eval_loss = None\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    fine_tune(model, train_dataloader, optimizer, criterion, BATCH_SIZE, device, SEQENCE_LENGTH)\n",
        "    _loss = evaluate(model, test_dataloader, optimizer, criterion, BATCH_SIZE, device, SEQENCE_LENGTH)\n",
        "    if eval_loss is not None and _loss > eval_loss:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       eval_loss = _loss\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid loss {:5.2f} | '.format(epoch, (time.time() - epoch_start_time),\n",
        "                                         _loss))\n",
        "    print('-' * 89)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}