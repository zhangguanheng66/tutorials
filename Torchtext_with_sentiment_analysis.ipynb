{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torchtext with sentiment analysis",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangguanheng66/tutorials/blob/sentiment_analysis/Torchtext_with_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMg-_Db4xil_"
      },
      "source": [
        "%%shell\n",
        "\n",
        "#rm -r /usr/local/lib/python3.6/dist-packages/torch*\n",
        "#pip install numpy\n",
        "pip uninstall torch torchtext\n",
        "pip install --pre torch torchtext -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjjwtY1Exr-e"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "import torchtext\n",
        "print(torchtext.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIBfeik3xzT-"
      },
      "source": [
        "# Sentiment Analysis with Torchtext\n",
        "\n",
        "[TODO] Add more details about the task and describe the work here, show how to prepare data.\n",
        "This tutorial is to show how to build the dataset to conduct sentiment analysis with torchtext library. The builiding blocks in torchtext library give the flexibility to build a custom data processing pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzujHlKmQvr1"
      },
      "source": [
        "## Step 1: Access to the raw dataset iterators\n",
        "----------------------------\n",
        "\n",
        "The torchtext library provides a few raw dataset iterators, which yield the raw text strings and labels. For example, the IMDB dataset iterators yield the raw data as a tuple of label and text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTTFwHimQunR"
      },
      "source": [
        "from torchtext.experimental.datasets.raw import IMDB\n",
        "train_iter, = IMDB(data_select=('train'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb0APgAhRO-P"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "next(iter(train_iter))\n",
        ">>> ('neg',\n",
        " 'I rented I AM CURIOUS-YELLOW from my video store because of all the \n",
        "controversy that surrounded it when it was first released in 1967. I also heard \n",
        "that at first it was seized by U.S. customs if it ever tried to enter this \n",
        "country, therefore being a fan of films considered \"controversial\" I really had \n",
        "to see this for myself.<br /><br />The plot is centered around a young Swedish \n",
        "drama student named Lena who wants to learn everything she can about life. In \n",
        "particular she wants to focus her attentions to making some sort of documentary \n",
        "on what the average Swede thought about certain political issues such as the \n",
        "Vietnam War and race issues in the United States. In between asking politicians \n",
        "and ordinary denizens of Stockholm about their opinions on politics, she has \n",
        "sex with her drama teacher, classmates, and married men.<br /><br />What kills \n",
        "me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered \n",
        "pornographic. Really, the sex and nudity scenes are few and far between, even \n",
        "then it\\'s not shot like some cheaply made porno. While my countrymen mind find \n",
        "it shocking, in reality sex and nudity are a major staple in Swedish cinema. \n",
        "Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex \n",
        "scenes in his films.<br /><br />I do commend the filmmakers for the fact that \n",
        "any sex shown in the film is shown for artistic purposes rather than just to \n",
        "shock people and make money to be shown in pornographic theaters in America. I \n",
        "AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and \n",
        "potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t \n",
        "have much of a plot.')\n",
        "\n",
        "next(iter(train_iter))\n",
        "('neg',\n",
        " '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It \n",
        "doesn\\'t matter what one\\'s political views are because this film can hardly be \n",
        "taken seriously on any level. As for the claim that frontal male nudity is an \n",
        "automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. \n",
        "Granted, they only offer some fleeting views, but where are the R-rated films \n",
        "with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The \n",
        "same goes for those crappy cable shows: schlongs swinging in the breeze but not \n",
        "a clitoris in sight. And those pretentious indie movies like The Brown Bunny, \n",
        "in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but \n",
        "not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \n",
        "\"double-standard\" in matters of nudity, the mentally obtuse should take into \n",
        "account one unavoidably obvious anatomical difference between men and women: \n",
        "there are no genitals on display when actresses appears nude, and the same \n",
        "cannot be said for a man. In fact, you generally won\\'t see female genitals in \n",
        "an American film in anything short of porn or explicit erotica. This alleged \n",
        "double-standard is less a double standard than an admittedly depressing ability \n",
        "to come to terms culturally with the insides of women\\'s bodies.')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0hKv5a9yBh2"
      },
      "source": [
        "## Step 2: Prepare data\n",
        "----------------------------\n",
        "We have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer backed by regular expression, and sentencepiece. Those are the basic data processing building blocks for raw text string.\n",
        "\n",
        "### Tokenizer-vocabulary data processing pipeline\n",
        "\n",
        "Here is an example for typical NLP data processing with tokenizer and vocabulary.\n",
        "\n",
        "Build vocab with the raw training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ8e0CPfNSBU"
      },
      "source": [
        "from torchtext.experimental.vocab import build_vocab_from_iterator\n",
        "from torchtext.experimental.transforms import basic_english_normalize\n",
        "tokenizer = basic_english_normalize()\n",
        "train_iter, = IMDB(data_select=('train'))\n",
        "vocab = build_vocab_from_iterator(iter(tokenizer(line) for label, line in train_iter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skoJqORdQDdM"
      },
      "source": [
        "```\n",
        "vocab(['here', 'is', 'an', 'example'])\n",
        ">>> [131, 9, 40, 464]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBhnX9u-ya6T"
      },
      "source": [
        "Prepare data pipeline for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC4gaPr5ybzQ"
      },
      "source": [
        "def generate_text_pipeline(tokenizer, vocab):\n",
        "  def _forward(text):\n",
        "    return vocab(tokenizer(text))\n",
        "  return _forward\n",
        "text_pipeline = generate_text_pipeline(basic_english_normalize(), vocab)\n",
        "label_pipeline = lambda x: 1 if x == 'pos' else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsZYFwWiUKJ_"
      },
      "source": [
        "The text piple converts a text string into a list of integers based on the lookup defined in the vocab. The label pipeline converts the label into integers. For example,\n",
        "\n",
        "```\n",
        "text_pipeline('here is the an example')\n",
        ">>> [131, 9, 1, 40, 464]\n",
        "label_pipeline('pos')\n",
        ">>> 1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PllfOE8-y7ye"
      },
      "source": [
        "### (Optional for tutorial) Tokenizer + Vocab + Embedding data processing pipeline\n",
        "\n",
        "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. FastText and GloVe are well established baseline word vectors in the NLP community. In the new torchtext library, a Vector object supports the mapping between tokens and their corresponding vector representation (i.e. word embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE7D5dmdy89K"
      },
      "source": [
        "from torchtext.experimental.vectors import FastText\n",
        "def generate_vector_pipeline(tokenizer, vector):\n",
        "  def _forward(text):\n",
        "    return vector(tokenizer(text))\n",
        "  return _forward\n",
        "word_vector_pipeline = generate_vector_pipeline(basic_english_normalize(), FastText())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iZ5wFUMk0OW"
      },
      "source": [
        "The word_vector_pipeline tokenizes a text string and converts the tokenzers into a vector, according to the pretrained word vector.\n",
        "\n",
        "```\n",
        "word_vector_pipeline('here is the an example')\n",
        ">>> tensor([[-0.1564,  0.0486,  0.1724,  ...,  0.4588, -0.0021,  0.3085],\n",
        "            [ 0.0359,  0.1452,  0.1193,  ..., -0.0016,  0.1708, -0.0355],\n",
        "            [-0.0653, -0.0930, -0.0176,  ...,  0.1664, -0.1308,  0.0354],\n",
        "            [-0.0671,  0.0014, -0.1857,  ...,  0.1050, -0.2144,  0.0944],\n",
        "            [ 0.0144,  0.1337, -0.1489,  ..., -0.0202,  0.0657, -0.0029]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLNPAmG-zBtC"
      },
      "source": [
        "### SentencePiece data processing pipeline\n",
        "\n",
        "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. For sentencepiece transforms in torchtext, both subword units (e.g., byte-pair-encoding (BPE) ) and unigram language model are supported. Here is an example to apply SentencePiece transform to build the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_EMgLIfzScN"
      },
      "source": [
        "from torchtext.experimental.transforms import (\n",
        "    PRETRAINED_SP_MODEL,\n",
        "    sentencepiece_processor,\n",
        "    load_sp_model,\n",
        ")\n",
        "from torchtext.utils import download_from_url\n",
        "spm_filepath = download_from_url(PRETRAINED_SP_MODEL['text_unigram_25000'])\n",
        "spm_transform = sentencepiece_processor(spm_filepath)\n",
        "sp_model = load_sp_model(spm_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXtEz6DJ2i7M"
      },
      "source": [
        "## Step 3: Data IteratorÂ¶\n",
        "\n",
        "The PyTorch data loading utility is the `torch.utils.data.DataLoader` class. It works with a map-style dataset that implements the `getitem()` and `len()` protocols, and represents a map from indices/keys to data samples. Before sending to the model, `collate_fn` function works on a batch of samples generated from DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvWw9sVE2idq"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# [TODO] integrate with torchtext.experimental.transforms.PadTransform\n",
        "# Need to land https://github.com/pytorch/text/pull/952\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "cls_id = sp_model.PieceToId('<cls>')\n",
        "pad_id = sp_model.PieceToId('<pad>')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        text_list.append(torch.tensor([cls_id] + spm_transform(_text)))\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=float(pad_id))\n",
        "    label_list = torch.tensor(label_list)\n",
        "    return label_list.to(device), text_list.transpose(0, 1).contiguous().to(device)\n",
        "\n",
        "train_iter, = IMDB(data_select=('train'))\n",
        "dataloader = DataLoader(list(train_iter), batch_size=8, shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNbi1He69ANc"
      },
      "source": [
        "## Step 4: Model for Sentiment Analysis Task\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLJYfMlq9HjU"
      },
      "source": [
        "from torch import nn\n",
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class SentimentAnalysisModel(nn.Module):\n",
        "    \"\"\"Contain a transformer encoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(SentimentAnalysisModel, self).__init__()\n",
        "        self.embed_layer = nn.Embedding(ntoken, ninp)\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.projection = nn.Linear(ninp, 2)\n",
        "\n",
        "    def forward(self, src_seq):\n",
        "        output = self.embed_layer(src_seq)\n",
        "        output = self.pos_encoder(output)\n",
        "        output = self.transformer_encoder(output)\n",
        "        output = self.activation(output[0])\n",
        "        return self.projection(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfyBKOxg_kg5"
      },
      "source": [
        "vocab_size = sp_model.GetPieceSize()\n",
        "emsize, nhead, nhid, nlayers, dropout = 64, 8, 128, 1, 0.2\n",
        "model = SentimentAnalysisModel(vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iovYWRYFBHnc"
      },
      "source": [
        "\n",
        "## Step 5: Train and test the Model\n",
        "---\n",
        "\n",
        "Then, we train and test the transformer model with the sentiment analysis based on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngQ1bnGlBZ0k"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def fine_tune(model, dataloader, optimizer, criterion, batch_size, device, SEQENCE_LENGTH):\n",
        "    model.train()\n",
        "    total_loss, total_acc, total_count = 0, 0, 0\n",
        "    log_interval = 50\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        # print(seq_input.size(), tok_type.size())\n",
        "        if text.size(0) > SEQENCE_LENGTH:\n",
        "            text = text[:SEQENCE_LENGTH]\n",
        "        predited_label = model(text)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += text.size(1)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | '\n",
        "                  'ms/batch {:5.2f} | loss {:5.2f} '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              scheduler.get_last_lr()[0],\n",
        "                                              elapsed * 1000 / log_interval,\n",
        "                                              cur_loss, total_acc/total_count))\n",
        "            total_loss, total_acc, total_count = 0, 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model, dataloader, optimizer,\n",
        "             criterion, batch_size, device, SEQENCE_LENGTH):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, total_count = 0, 0, 0\n",
        "    ans_pred_tokens_samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            if text.size(0) > SEQENCE_LENGTH:\n",
        "              text = text[:SEQENCE_LENGTH]\n",
        "            predited_label = model(text)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_loss += loss.item()\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += text.size(1)\n",
        "    return total_loss / len(dataloader), total_acc/total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSBskbHkQ3Ll"
      },
      "source": [
        "Here are a few hyperparameters used in the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXD1I31kCE2T"
      },
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "SEQENCE_LENGTH = 1025 # the maximum sequence length\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "eval_loss = None\n",
        "train_iter, test_iter = IMDB()\n",
        "train_dataloader = DataLoader(list(train_iter), batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(list(test_iter), batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    fine_tune(model, train_dataloader, optimizer,\n",
        "              criterion, BATCH_SIZE, device, SEQENCE_LENGTH)\n",
        "    loss_val, accu_val = evaluate(model, test_dataloader, optimizer,\n",
        "                                  criterion, BATCH_SIZE, device, SEQENCE_LENGTH)\n",
        "    if eval_loss is not None and loss_val > eval_loss:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       eval_loss = loss_val\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid loss {:5.2f} | valid accuracy {:8.3f} '.format(epoch,\n",
        "                                         (time.time() - epoch_start_time),\n",
        "                                         loss_val, accu_val))\n",
        "    print('-' * 89)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}