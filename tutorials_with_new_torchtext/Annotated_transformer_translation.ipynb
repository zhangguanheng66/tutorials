{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Annotated_transformer_translation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMH5sBr33U/jbjwnCDzyiz9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangguanheng66/tutorials/blob/tutorials_with_new_torchtext/tutorials_with_new_torchtext/Annotated_transformer_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeHecm-OG9N0"
      },
      "source": [
        "%%shell\n",
        "rm -r /usr/local/lib/python3.6/dist-packages/torch*\n",
        "pip install --pre torch torchvision torchtext -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdL90YgqQh_B"
      },
      "source": [
        "import torchtext\n",
        "print(torchtext.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3bbwpYXHgIR"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0sLx2A8aFAQ"
      },
      "source": [
        "from torch.nn import Transformer, Embedding\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)\n",
        "        \n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class AnnotatedTransformer(nn.Module):\n",
        "  def __init__(self, src_vocab, tgt_vocab,\n",
        "               N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    super(AnnotatedTransformer, self).__init__()\n",
        "    self.transformer = Transformer(d_model=d_model, nhead=h,\n",
        "                                   num_encoder_layers=N, num_decoder_layers=N,\n",
        "                                   dim_feedforward=d_ff, dropout=dropout)\n",
        "    self.d_model = d_model\n",
        "    self.src_embed = Embedding(src_vocab, d_model)\n",
        "    self.src_pos_embed = PositionalEncoding(d_model, dropout)\n",
        "    self.tgt_embed = Embedding(tgt_vocab, d_model)\n",
        "    self.tgt_pos_embed = PositionalEncoding(d_model, dropout)\n",
        "    self.generator = Generator(d_model, tgt_vocab)\n",
        "\n",
        "  def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "    src = self.src_pos_embed(self.src_embed(src) * math.sqrt(self.d_model))\n",
        "    tgt = self.tgt_pos_embed(self.tgt_embed(tgt) * math.sqrt(self.d_model))\n",
        "    # print(src.is_cuda, tgt.is_cuda, src_mask.is_cuda, tgt_mask.is_cuda)\n",
        "    out = self.transformer(src, tgt, src_mask, tgt_mask)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnc40PmuJYQG"
      },
      "source": [
        "def run_epoch(data_iter, model, criterion, optimizer=None):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "\n",
        "    for i, (src, src_mask, tgt, tgt_mask) in enumerate(data_iter):\n",
        "        ntokens = src.size(0) * src.size(1)\n",
        "        out = model(src, tgt, src_mask, tgt_mask)\n",
        "        out = model.generator(out)\n",
        "        loss = criterion(out.contiguous().view(-1, out.size(-1)), \n",
        "                         src.contiguous().view(-1)) / src.size(1)\n",
        "\n",
        "        if optimizer is not None:\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_tokens += ntokens\n",
        "        tokens += ntokens\n",
        "        if i % 50 == 49:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %12.9f Tokens per Sec: %f\" %\n",
        "                    (i, loss / ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7MtS-fyep_T"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def data_gen(V, batch, num_samples, num_words):\n",
        "    \"Generate random data for a src-tgt copy task.\"\n",
        "    sample_data = []\n",
        "    for i in range(num_samples):\n",
        "        data = torch.randint(1, V, size=(num_words, batch))\n",
        "        data[0, :] = 0\n",
        "        src = data[1:, :]\n",
        "        tgt = data[:-1, :]\n",
        "        sample_data.append((src, tgt))\n",
        "    return sample_data\n",
        "\n",
        "V, batch, nsamples, nwords = 11, 64, 300, 15\n",
        "model = AnnotatedTransformer(V, V, N=2).to(device)\n",
        "subsequent_mask = model.transformer.generate_square_subsequent_mask\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.8)\n",
        "\n",
        "def genereate_batch(raw_data):\n",
        "  src, tgt = raw_data[0]\n",
        "  src_mask = subsequent_mask(src.size(0)).to(device)\n",
        "  tgt_mask = subsequent_mask(tgt.size(0)).to(device)\n",
        "  return src.to(device), src_mask, tgt.to(device), tgt_mask\n",
        "\n",
        "train_data = torch.utils.data.DataLoader(data_gen(V, batch, nsamples, nwords), \n",
        "                                         collate_fn=genereate_batch)\n",
        "test_data = torch.utils.data.DataLoader(data_gen(V, batch, 5, nwords), \n",
        "                                        collate_fn=genereate_batch)\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    run_epoch(train_data, model, criterion, optimizer)\n",
        "\n",
        "    # Adjust the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    print(run_epoch(test_data, model, criterion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNdDvhPtJ2Vt"
      },
      "source": [
        "model = model.to(\"cpu\")\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    ys = torch.ones(1, src.size(1), dtype=torch.long).fill_(start_symbol).to(src.device)\n",
        "    for i in range(max_len-1):\n",
        "        ys_mask = subsequent_mask(ys.size(0)).to(src.device)\n",
        "        # print(src.is_cuda, ys.is_cuda, src_mask.is_cuda, ys_mask.is_cuda)\n",
        "        out = model(src, ys, src_mask, ys_mask)\n",
        "        prob = model.generator(out[-1, :])\n",
        "        next_word = prob[-1, :].argmax(-1).item()\n",
        "        ys = torch.cat([ys, torch.ones(1, src.size(1), dtype=torch.long).fill_(next_word).to(ys.device)], dim=0)\n",
        "    return ys\n",
        "\n",
        "model.eval()\n",
        "src = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=torch.long).t()\n",
        "src_mask = subsequent_mask(src.size(0))\n",
        "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0maRfqqYs9k"
      },
      "source": [
        "%%shell\n",
        "python -m spacy download en\n",
        "python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zebcpgX3NF2S"
      },
      "source": [
        "%%shell\n",
        "wget https://pytorch.s3.amazonaws.com/models/text/annotated_transformer_en_vocab.txt\n",
        "wget https://pytorch.s3.amazonaws.com/models/text/annotated_transformer_de_vocab.txt\n",
        "wget https://pytorch.s3.amazonaws.com/models/text/annotated_torch_transformer.pt\n",
        "wget https://s3.amazonaws.com/opennmt-models/iwslt.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIaAQHN-M8S6"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.experimental.datasets.raw import IWSLT\n",
        "from torchtext.experimental.vocab import load_vocab_from_file\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "f = open('annotated_transformer_de_vocab.txt', 'r')\n",
        "de_vocab = load_vocab_from_file(f)\n",
        "de_tokenizer = get_tokenizer(\"spacy\", language='de')\n",
        "\n",
        "f = open('annotated_transformer_en_vocab.txt', 'r')\n",
        "en_vocab = load_vocab_from_file(f)\n",
        "en_tokenizer = get_tokenizer(\"spacy\", language='en')\n",
        "\n",
        "def data_process(de_raw, en_raw):\n",
        "  return (de_vocab(de_tokenizer(de_raw)), en_vocab(en_tokenizer(en_raw)))\n",
        "\n",
        "raw_train_iter, raw_valid_iter, raw_test_iter = IWSLT()\n",
        "processed_data = [data_process(de_raw, en_raw) for (de_raw, en_raw) in raw_train_iter]\n",
        "pad_idx = en_vocab(['<pad>'])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG5pMsUD25ky"
      },
      "source": [
        "# Use a pretrained torch.nn.Transformer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.load(\"annotated_torch_transformer.pt\")\n",
        "model = model.to(device)\n",
        "\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    ys = torch.ones(1, src.size(1), dtype=torch.long).fill_(start_symbol).to(src.device)\n",
        "    for i in range(max_len-1):\n",
        "        ys_mask = subsequent_mask(ys.size(0)).to(src.device)\n",
        "        out = model(src, ys, src_mask, ys_mask)\n",
        "        prob = model.generator(out[-1, :])\n",
        "        next_word = prob[-1, :].argmax(-1).item()\n",
        "        ys = torch.cat([ys, torch.ones(1, src.size(1), dtype=torch.long).fill_(next_word).to(ys.device)], dim=0)\n",
        "    return ys\n",
        "\n",
        "def collate_func(data_batch):\n",
        "  de_data_, en_data_ = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_data_.append(torch.tensor(de_item, dtype=torch.long))\n",
        "    en_data_.append(torch.tensor(en_item, dtype=torch.long))\n",
        "  de_data_ = pad_sequence(de_data_, padding_value=pad_idx).long()\n",
        "  en_data_ = pad_sequence(en_data_, padding_value=pad_idx).long()\n",
        "  return de_data_, en_data_\n",
        "\n",
        "train_dataloader = DataLoader(processed_data, batch_size=8, shuffle=False, collate_fn=collate_func)\n",
        "count = 0\n",
        "for (de_data, en_data) in train_dataloader:\n",
        "  src, tgt = de_data[0].unsqueeze(0).transpose(0, 1).to(device), en_data[0].unsqueeze(0).transpose(0, 1).to(device)\n",
        "  src_mask = subsequent_mask(src.size(0)).to(device)\n",
        "  out = greedy_decode(model, src, src_mask, max_len=60, \n",
        "                      start_symbol=en_vocab(['<s>'])[0])\n",
        "  print('-------------------------------', count)\n",
        "  count += 1\n",
        "  print(\"Source:\", \" \".join(de_vocab.lookup_tokens(src.t()[0].tolist())))\n",
        "  print(\"Target:\", \" \".join(en_vocab.lookup_tokens(tgt.t()[0].tolist())))\n",
        "  print(\"Translation:\", \" \".join(en_vocab.lookup_tokens(out.t()[0].tolist())))\n",
        "\n",
        "  if count > 2:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOeuB8kN0Qm1"
      },
      "source": [
        "# Use the pretrained Transformer model from Annotated Transformer tutorial\n",
        "from annotated_transformer import EncoderDecoder, Encoder, EncoderLayer, MultiHeadedAttention, \\\n",
        "  PositionwiseFeedForward, SublayerConnection, LayerNorm, Decoder, \\\n",
        "  DecoderLayer, Embeddings\n",
        "  \n",
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "model = torch.load(\"iwslt.pt\")\n",
        "device = torch.device('cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys\n",
        "\n",
        "def collate_func(data_batch):\n",
        "  de_data_, en_data_ = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_data_.append(torch.tensor(de_item, dtype=torch.long))\n",
        "    en_data_.append(torch.tensor(en_item, dtype=torch.long))\n",
        "  de_data_ = pad_sequence(de_data_, padding_value=pad_idx).long()\n",
        "  en_data_ = pad_sequence(en_data_, padding_value=pad_idx).long()\n",
        "  return de_data_, en_data_\n",
        "\n",
        "train_dataloader = DataLoader(processed_data, batch_size=8, shuffle=False, collate_fn=collate_func)\n",
        "count = 0\n",
        "for (de_data, en_data) in train_dataloader:\n",
        "  de_data, en_data = de_data.transpose(0, 1), en_data.transpose(0, 1)\n",
        "  src, tgt = de_data[0].unsqueeze(0).to(device), en_data[0].unsqueeze(0).to(device)\n",
        "  src_mask = (src != de_vocab(['<pad>'])[0]).unsqueeze(0).to(device)\n",
        "  out = greedy_decode(model, src, src_mask, max_len=60, \n",
        "                      start_symbol=en_vocab(['<s>'])[0])\n",
        "  print('-------------------------------', count)\n",
        "  count += 1\n",
        "  print(\"Source:\", \" \".join(de_vocab.lookup_tokens(src[0].tolist())))\n",
        "  print(\"Target:\", \" \".join(en_vocab.lookup_tokens(tgt[0].tolist())))\n",
        "  print(\"Translation:\", \" \".join(en_vocab.lookup_tokens(out[0].tolist())))\n",
        "\n",
        "  if count > 2:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}